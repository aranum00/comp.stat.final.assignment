---
title: "tidymodels_assignment"
output: pdf_document
date: "2025-06-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Below we show the procedure that follows lecture 1. We've done the core tidymodels workflow using ames dataset covering model specification using parsnip, data preprocessing with recipes, workflow creation with workflows, model fitting, and prediction/evaluation.
```{r, warning = FALSE}
# Load packages
library(tidymodels)
library(readr)
library(modeldata)
library(ggplot2)

# Load the ames dataset
data(ames)

# Visualize sale price vs. living area by neighborhood
ggplot(
  ames,
  aes(
    x = Gr_Liv_Area,
    y = Sale_Price,
    color = Neighborhood
  )
) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7) +
  labs(title = "Sale Price vs. Living Area by Neighborhood",
       x = "Ground Living Area",
       y = "Sale Price")
```

Model specification (using parsnip)
```{r}
# Define a linear regression model
lm_mod_ames <- linear_reg()

# Fit the model with interaction term
lm_fit_ames <- lm_mod_ames %>%
  fit(Sale_Price ~ Gr_Liv_Area * Year_Built,
      data = ames)

lm_fit_ames

# Summarize coeffs
summary(lm_fit_ames)
tidy(lm_fit_ames)

lm_fit_ames$fit
```


Prediction using fitted model
```{r}
# Create new data points for prediction
new_house_points <- expand.grid(
  Gr_Liv_Area = c(1500, 2000, 2500),
  Year_Built = c(1980, 2000, 2020)
)

# Predict mean sale prices for new data
mean_pred_ames <- predict(lm_fit_ames,
                          new_data = new_house_points)
mean_pred_ames

# Predict CIs for sale prices
conf_int_pred_ames <- predict(lm_fit_ames,
                              new_data = new_house_points,
                              type = "conf_int")
conf_int_pred_ames
```


Data preprocessing with "recipes" package
```{r}
set.seed(1)

# Split data into training and testing data, stratifying by sale_price
data_split_ames <- initial_split(ames, prop = 3/4, strata = Sale_Price)
train_data_ames <- training(data_split_ames)
test_data_ames <- testing(data_split_ames)

# Creating recipe
ames_rec <- recipe(Sale_Price ~ ., data = train_data_ames)
summary(ames_rec)


# Add steps to recipe
ames_rec <- ames_rec %>%
  step_mutate(
    Year_Built = factor(Year_Built),
    Mo_Sold = factor(Mo_Sold)
  ) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())
```


Merging recipe and model with "workflows"
```{r}
# Define a linear regression model for workflow
lm_mod_ames_workflow <- linear_reg() %>%
  set_engine("lm")

# Create a workflow by adding model and recipe
ames_wflow <- workflow() %>%
  add_model(lm_mod_ames_workflow) %>%
  add_recipe(ames_rec)

ames_wflow

# Fit workflow to training data
flights_fit_ames <- ames_wflow %>%
  fit(data = train_data_ames)

# Tidy fitted model's coeffs
flights_fit_ames %>%
  extract_fit_parsnip() %>%
  tidy()
```


Workflow prediction and performance measurement
```{r, warning = FALSE}
# Predict sale prices on test data using fitted workflow
predict(flights_fit_ames, test_data_ames)

# Augment test data with predictions and probabilities
ames_aug <- augment(flights_fit_ames, test_data_ames)
ames_aug

# Calculate RMSE
ames_aug %>%
  rmse(truth = Sale_Price, .pred)

# Calculate R-squared
ames_aug %>%
  rsq(truth = Sale_Price, .pred)

# Plot actual vs. predicted sale prices
ggplot(ames_aug, aes(x = Sale_Price, y = .pred)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs. Predicted Sale Price (Test Data)",
       x = "Actual Sale Price",
       y = "Predicted Sale Price") +
  theme_minimal()
```








Below we show the procedure that follows lecture 2. This part we continue, focusing on evaluating and tuning classification models using the attrition dataset. The goal is to predict if an employee will leave. (Attrition yes/no)
The process involves splitting data into training and testing sets, defining a model and a data preprocessing recipe, then combining them into a workflow for initial fitting and evaluation. For robust assessment and optimization, cross-validation is used for resampling and hyperparameter tuning, used in a final model fit and performance evaluation on test data.

Loading data
```{r}
# attrition data
library(modeldata)

data(attrition, package = "modeldata")

# Make attrition is factor
attrition <- attrition %>%
  mutate(Attrition = factor(Attrition, levels = c("Yes", "No")))
```

Splitting data
```{r}
set.seed(1)
attrition_split <- initial_split(attrition,
                                 strata = Attrition
)

# Training and test data
attrition_train <- training(attrition_split)
attrition_test <- testing(attrition_split)

# Check stratification proportions in training set
attrition_train %>%
  count(Attrition) %>%
  mutate(prop = n / sum(n))

# Check stratification proportions in test set
attrition_test %>%
  count(Attrition) %>%
  mutate(prop = n / sum(n))
```

Model with random forest
```{r}
# Define random forest model for classification
rf_mod <- rand_forest(mode = "classification", trees = 1000) %>%
  set_engine("ranger")

# Fitting Random Forest model to training data
set.seed(234)
rf_fit <- rf_mod %>%
  fit(Attrition ~ ., data = attrition_train)

rf_fit
```

Looking at training and test errors
```{r}
# Make predictions on training data
rf_training_pred <- predict(rf_fit, attrition_train) %>%
  bind_cols(predict(rf_fit, attrition_train, type = "prob")) %>%
  bind_cols(select(attrition_train, Attrition))

# Make predictions on test data
rf_testing_pred <- predict(rf_fit, attrition_test) %>%
  bind_cols(predict(rf_fit, attrition_test, type = "prob")) %>%
  bind_cols(select(attrition_test, Attrition))

# Calculate ROC/AUC on test data
rf_testing_pred %>%
  roc_auc(truth = Attrition, .pred_Yes)

# Calculate accuracy on training data
rf_training_pred %>%
  accuracy(truth = Attrition, .pred_class)

# Calculate accuracy on test data
rf_testing_pred %>%
  accuracy(truth = Attrition, .pred_class)
```

Resampling procedure using cross-validation
```{r}
set.seed(1)
# Create 10-fold CV folds on training data
folds <- vfold_cv(attrition_train, v = 10, strata = Attrition) # Stratify folds
folds
```

Fitting on folds
```{r}
# Create workflow for random forest model
rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_formula(Attrition ~ .)

# Fit workflow to the CV folds
set.seed(456)
rf_fit_rs <- rf_wf %>%
  fit_resamples(folds)

# Performance metrics from CV
collect_metrics(rf_fit_rs)
```

Tuning hyperparameters
```{r}
# Defining decision tree model with tunable hyperparameters
tune_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tune_spec

# Creating regular grid of hyperparameter combinations
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
tree_grid

# Set up CV folds for tuning
set.seed(1)
attrition_folds <- vfold_cv(attrition_train, strata = Attrition)
```


Fitting for each combination (tuning)
```{r}
# Create workflow for tunable decision tree
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(Attrition ~ .)

# Tune the model using tune_grid across the CV folds
set.seed(1)
tree_res <- tree_wf %>%
  tune_grid(
    resamples = attrition_folds,
    grid = tree_grid
  )

collect_metrics(tree_res)

# Plot tuning results across hyperparameters
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```


We select the "best" model
```{r}
# Show the top 5 best models according to accuracy
tree_res %>%
  show_best(metric = "accuracy")

# Selecting best hyperparameter combination
best_tree <- tree_res %>%
  select_best(metric = "accuracy")
best_tree

# We finalize workflow with the best hyperparameters
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf
```


Using the "best" model for our final evaluation
```{r}
# Performing final fit on training data and evaluate on test data
final_fit <- final_wf %>%
  last_fit(attrition_split)

# Performance metrics
final_fit %>%
  collect_metrics()

# Plotting ROC curve
final_fit %>%
  collect_predictions() %>%
  roc_curve(Attrition, .pred_Yes) %>%
  autoplot()
```

